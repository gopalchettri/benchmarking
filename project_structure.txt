llm-benchmarking/                           # Root: LLM Benchmarking for UAE IA Cybersecurity Use Cases
│
├── .venv/                                  # Virtual environment (auto-managed by UV, DO NOT commit)
│   ├── bin/                                # Python executables and scripts
│   ├── lib/                                # Installed packages and dependencies
│   └── pyvenv.cfg                          # Virtual environment configuration
│
├── .gitignore                              # Git exclusions: .venv/, results/, __pycache__, *.pyc, logs/
├── .python-version                         # Python version for this project (e.g., 3.11.9)
├── README.md                               # Project overview, setup guide, usage instructions
├── pyproject.toml                          # UV project config: metadata, dependencies, scripts
├── uv.lock                                 # Dependency lockfile for reproducible installs (auto-generated)
│
├── main.py                                 # CLI entry point: Execute benchmarks for specific use cases
│                                          # Example: python main.py criteria_generation
│
├── src/                                    # Source code: Core logic, use cases, APIs
│   ├── __init__.py                         # Package initialization
│   │
│   ├── core/                               # Shared utilities used across all 4 use cases
│   │   ├── __init__.py                     
│   │   │
│   │   ├── model_client.py                # LLM inference client: Handles API calls to 6 models
│   │   │                                   # - Manages authentication, rate limiting, retries
│   │   │                                   # - Supports: phi-4, falcon-2, GPT-OSS, Llama 3.2, Jais, K2-think
│   │   │                                   # - Functions: generate_response(model, prompt, params)
│   │   │
│   │   ├── database.py                     # Database connections and session management
│   │   │                                   # - MongoDB: Stores raw JSON outputs from LLM (pymongo/motor)
│   │   │                                   # - MySQL: Stores structured metrics, test executions (pymysql)
│   │   │                                   # - Neo4j: Graphs control hierarchies (neo4j driver)
│   │   │                                   # - Functions: init_databases(), get_mongo_client(), get_mysql_conn()
│   │   │
│   │   ├── metrics.py                      # Evaluation metrics calculation
│   │   │                                   # - Scores: relevance, specificity, policy_level_focus, completeness
│   │   │                                   # - Common scoring logic across all use cases (0-10 scale)
│   │   │                                   # - Functions: calculate_relevance(), calculate_specificity()
│   │   │
│   │   └── utils.py                        # Helper functions
│   │                                       # - Logging setup, file I/O, timestamp generation
│   │                                       # - Functions: setup_logger(), save_json(), generate_uuid()
│   │
│   ├── usecases/                           # Four benchmarking use cases (modular design)
│   │   │
│   │   ├── criteria_generation/           # Use Case 1: Generate evaluation criteria from controls
│   │   │   ├── __init__.py
│   │   │   │
│   │   │   ├── benchmark.py                # Test executor for Criteria Generation
│   │   │   │                               # - Loads 25 test cases from config/test_cases_uc1.json
│   │   │   │                               # - Runs 6 models x parameter combinations (temp, top_p, etc.)
│   │   │   │                               # - Stores outputs in MongoDB, metrics in MySQL
│   │   │   │                               # - Main function: run_criteria_benchmark()
│   │   │   │
│   │   │   ├── evaluator.py                # Use case-specific evaluation logic
│   │   │   │                               # - Validates criteria are policy-level (not execution evidence)
│   │   │   │                               # - Checks precision/identifiability of generated criteria
│   │   │   │                               # - Functions: evaluate_criteria_quality(), check_policy_level()
│   │   │   │
│   │   │   └── prompts.py                  # Prompt templates for criteria generation
│   │   │                                   # - System prompts, few-shot examples for each model
│   │   │                                   # - Functions: get_criteria_prompt(control_desc, framework)
│   │   │
│   │   ├── question_generation/           # Use Case 2: Generate questions based on criteria
│   │   │   ├── __init__.py
│   │   │   │
│   │   │   ├── benchmark.py                # Test executor for Question Generation
│   │   │   │                               # - Loads criteria from Use Case 1 results
│   │   │   │                               # - Generates questions for 20 test cases x 6 models
│   │   │   │                               # - Main function: run_question_benchmark()
│   │   │   │
│   │   │   ├── evaluator.py                # Evaluates question quality
│   │   │   │                               # - Checks clarity, relevance to criteria, answerability
│   │   │   │                               # - Functions: evaluate_question_quality()
│   │   │   │
│   │   │   └── prompts.py                  # Question generation prompts
│   │   │
│   │   ├── evidence_analyzer/             # Use Case 3: Analyze evidence against criteria
│   │   │   ├── __init__.py
│   │   │   │
│   │   │   ├── benchmark.py                # Test executor for Evidence Analyzer
│   │   │   │                               # - Processes evidence documents (20 test cases)
│   │   │   │                               # - Evaluates compliance vs. non-compliance
│   │   │   │                               # - Generates gap observations and recommendations
│   │   │   │                               # - Main function: run_evidence_benchmark()
│   │   │   │
│   │   │   ├── doc_parser.py               # Document parsing with docling and vision models
│   │   │   │                               # - Extracts text from evidence documents (PDF, Word, images)
│   │   │   │                               # - Uses Llama 3.2 11B vision for image-based evidence
│   │   │   │                               # - Functions: parse_document(), extract_text_with_vision()
│   │   │   │
│   │   │   ├── evaluator.py                # Evaluates gap analysis quality
│   │   │   │                               # - Validates compliant/non-compliant classification accuracy
│   │   │   │                               # - Checks recommendation actionability
│   │   │   │                               # - Functions: evaluate_gap_analysis()
│   │   │   │
│   │   │   └── prompts.py                  # Evidence analysis prompts
│   │   │
│   │   └── policy_chatbot/                # Use Case 4: RAG-based policy chatbot
│   │       ├── __init__.py
│   │       │
│   │       ├── benchmark.py                # Test executor for Policy Chatbot
│   │       │                               # - Runs 20 query test cases x 6 models
│   │       │                               # - Evaluates retrieval relevance and answer quality
│   │       │                               # - Main function: run_chatbot_benchmark()
│   │       │
│   │       ├── rag_handler.py              # RAG pipeline implementation
│   │       │                               # - Vector database setup (e.g., Pinecone, ChromaDB)
│   │       │                               # - Embeddings for policies/framework docs
│   │       │                               # - Retrieval and context augmentation
│   │       │                               # - Functions: initialize_vector_db(), retrieve_context()
│   │       │
│   │       ├── evaluator.py                # Evaluates RAG answer quality
│   │       │                               # - Checks answer accuracy, context relevance
│   │       │                               # - Functions: evaluate_rag_response()
│   │       │
│   │       └── prompts.py                  # RAG chatbot prompts
│   │
│   ├── api/                                # FastAPI REST services for benchmarking
│   │   ├── __init__.py
│   │   │
│   │   ├── main.py                         # FastAPI app entry point
│   │   │                                   # - Defines app, middleware, exception handlers
│   │   │                                   # - Serves endpoints: /benchmark, /results, /health
│   │   │                                   # - Run with: uvicorn src.api.main:app --reload
│   │   │
│   │   ├── routers/                        # API route modules for each use case
│   │   │   ├── __init__.py
│   │   │   │
│   │   │   ├── criteria.py                 # POST /benchmark/criteria - Run criteria gen tests
│   │   │   │                               # GET /results/criteria/{execution_id} - Fetch results
│   │   │   │
│   │   │   ├── questions.py                # Endpoints for question generation benchmarking
│   │   │   ├── evidence.py                 # Endpoints for evidence analyzer benchmarking
│   │   │   └── chatbot.py                  # Endpoints for policy chatbot benchmarking
│   │   │
│   │   ├── models.py                       # Pydantic models for request/response schemas
│   │   │                                   # - BenchmarkRequest, TestCaseInput, BenchmarkResult
│   │   │                                   # - Validates API payloads, ensures type safety
│   │   │
│   │   └── dependencies.py                 # Shared API dependencies
│   │                                       # - Database session providers (MongoDB, MySQL)
│   │                                       # - Authentication/authorization (if needed)
│   │                                       # - Functions: get_db_session(), verify_api_key()
│   │
│   └── config/                             # Configuration files (JSON format)
│       ├── __init__.py
│       │
│       ├── test_cases_uc1.json             # 25 test cases for Use Case 1 (Criteria Generation)
│       │                                   # - UAE IA controls: control_id, description, priority
│       │                                   # - Example: {control_id: "M1.1", description: "...", priority: "P1"}
│       │
│       ├── test_cases_uc2.json             # 20 test cases for Use Case 2 (Question Generation)
│       │
│       ├── test_cases_uc3.json             # 20 test cases for Use Case 3 (Evidence Analyzer)
│       │                                   # - Includes evidence file paths, expected compliance status
│       │
│       ├── test_cases_uc4.json             # 20 test cases for Use Case 4 (Policy Chatbot)
│       │                                   # - User queries and expected answer types
│       │
│       ├── models.json                     # Model configurations for 6 LLMs
│       │                                   # - API endpoints, authentication tokens, default params
│       │                                   # - Example: {name: "phi-4", api_url: "...", auth_token: "..."}
│       │
│       ├── parameters.json                 # Parameter combinations for benchmarking
│       │                                   # - temp: [0.3, 0.5, 0.7], top_p: [0.85, 0.90, 0.95], etc.
│       │                                   # - Defines grid search for optimal params
│		│									
│		├── parameter_set.json              # 3 Parameter combo for benchmarking
│       │                                   # [{"temperature": 0.0, "top_p": 0.85, "top_k": 40, "min_p": 0.05, "repetition_penalty": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "context_window": 4096, "max_tokens": 512}, {"temperature": 0.3, "top_p": 0.90, "top_k": 50, "min_p": 0.05, "repetition_penalty": 1.1, "frequency_penalty": 0.2, "presence_penalty": 0.2, "context_window": 8192, "max_tokens": 1024}, {"temperature": 0.8, "top_p": 0.95, "top_k": 60, "min_p": 0.10, "repetition_penalty": 1.2, "frequency_penalty": 0.4, "presence_penalty": 0.4, "context_window": 16384, "max_tokens": 2048}]
│       │                                   #  3 recommended parameter combinations (grids) for 6-model benchmarking, each chosen to represent a distinct generation style—conservative, balanced, and creative/extreme.
│       │
│       └── database.json                   # Database connection strings
│                                           # - Local: mongodb://localhost:27017, mysql://localhost:3306
│                                           # - Azure: Azure Cosmos DB, Azure MySQL connection strings
│
├── tests/                                  # Pytest unit and integration tests
│   ├── __init__.py
│   │
│   ├── conftest.py                         # Pytest fixtures and shared test utilities
│   │                                       # - Mock database connections, sample test data
│   │                                       # - Fixtures: mock_mongo_client(), sample_control_data()
│   │
│   ├── test_model_client.py                # Unit tests for LLM client
│   │                                       # - Tests API calls, retry logic, error handling
│   │                                       # - Mocks external API responses
│   │
│   ├── test_database.py                    # Tests database connections and CRUD operations
│   │                                       # - MongoDB insert/query, MySQL transactions
│   │
│   ├── test_metrics.py                     # Tests evaluation metric calculations
│   │                                       # - Validates scoring functions (0-10 scale)
│   │
│   └── test_benchmarks/                    # Integration tests for each use case
│       ├── test_criteria_gen.py            # End-to-end test for criteria generation
│       ├── test_question_gen.py            # End-to-end test for question generation
│       ├── test_evidence_analyzer.py       # End-to-end test for evidence analysis
│       └── test_policy_chatbot.py          # End-to-end test for RAG chatbot
│
├── data/                                   # Test data and evidence documents
│   ├── frameworks/                         # Framework documents for reference
│   │   ├── UAE_IA_Standard_v2.pdf          # UAE IA framework specification
│   │   └── SAMA_Framework.pdf              # SAMA cybersecurity framework
│   │
│   ├── evidence/                           # Sample evidence documents for Use Case 3
│   │   ├── policy_001.pdf                  # Example: Information Security Policy
│   │   ├── procedure_002.docx              # Example: Access Control Procedure
│   │   └── screenshot_003.png              # Example: Configuration screenshot
│   │
│   └── knowledge_base/                     # Documents for RAG (Use Case 4)
│       ├── company_policies/               # Internal policies for chatbot
│       └── framework_docs/                 # Framework guidance documents
│
├── results/                                # Benchmarking outputs (DO NOT commit, add to .gitignore)
│   │
│   ├── logs/                               # Execution logs
│   │   ├── benchmark_20251103_155000.log   # Timestamped log files
│   │   └── error.log                       # Error logs for debugging
│   │
│   ├── raw_outputs/                        # Raw LLM responses per test case
│   │   ├── uc1_criteria/                   # Use Case 1 outputs
│   │   │   ├── phi4_tc001_temp0.5.json     # Model_TestCase_Params.json
│   │   │   └── falcon2_tc001_temp0.5.json
│   │   ├── uc2_questions/                  # Use Case 2 outputs
│   │   ├── uc3_evidence/                   # Use Case 3 outputs
│   │   └── uc4_chatbot/                    # Use Case 4 outputs
│   │
│   ├── metrics/                            # Evaluated metrics (CSV/JSON)
│   │   ├── uc1_metrics.csv                 # Metrics table: model, test_case, scores
│   │   ├── uc2_metrics.csv
│   │   ├── summary_report.json             # Aggregated results across all use cases
│   │   └── model_comparison.csv            # Comparative analysis: best model per use case
│   │
│   └── graphs/                             # Neo4j graph exports
│       ├── control_hierarchy.cypher        # Cypher queries to recreate graph
│       └── test_relationships.json         # JSON export of test case relationships
│
├── scripts/                                # Utility scripts for setup and maintenance
│   ├── setup_databases.py                  # Initialize MongoDB, MySQL, Neo4j schemas
│   │                                       # - Creates tables, collections, indexes
│   │                                       # - Run once before benchmarking
│   │
│   ├── export_results.py                   # Export benchmark results to Excel/CSV
│   │                                       # - Generates reports for stakeholders
│   │
│   └── migrate_to_azure.py                 # Update configs for Azure deployment
│                                           # - Replaces local connection strings with Azure
│
└── docs/                                   # Project documentation
    ├── architecture.md                     # System architecture diagrams
    ├── setup_guide.md                      # Detailed setup instructions
    ├── benchmarking_guide.md               # How to run benchmarks, interpret results
    ├── api_reference.md                    # FastAPI endpoint documentation
    └── evaluation_methodology.md           # Explanation of scoring criteria
