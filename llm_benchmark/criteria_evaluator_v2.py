# We import tools that help us run tasks at the same time (async), handle JSON,
# log messages, and define types for function inputs and outputs.
import asyncio
import json
import logging
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor

# We import NumPy, a library for working with numbers and vectors (lists of numbers).
import numpy as np

# We import SentenceTransformer, a library that turns sentences into numeric vectors
# so we can measure how similar two sentences are in meaning.
from sentence_transformers import SentenceTransformer

# -------------------------
# Configuration & Logging
# -------------------------

# Create a "logger" that we will use to print warnings or error messages.
logger = logging.getLogger(__name__)

# Tell Python to show log messages of level INFO and above in the console.
logging.basicConfig(level=logging.INFO)

# -------------------------
# Embedding model setup
# -------------------------

# Name of the sentence embedding model we will use.
# This model converts sentences into numeric vectors that capture their meaning.
# You can replace this with "all-mpnet-base-v2" in future for more accuracy.
MODEL_NAME = "all-MiniLM-L6-v2"

# We load the sentence embedding model into memory.
MODEL = SentenceTransformer(MODEL_NAME)

# We create a pool of worker threads to run heavy tasks (like embedding) without blocking.
EXECUTOR = ThreadPoolExecutor(max_workers=4)

# -------------------------
# Thresholds
# -------------------------

# If the similarity between LLM output and human text is below this value,
# we will treat it as "hallucinated" (too different from what was expected).
SIMILARITY_THRESHOLD = 0.65


# -------------------------
# Utility functions
# -------------------------

def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """
    This function measures how similar two numeric vectors are.
    Each vector represents a sentence.
    The result is between -1 and 1:
      - 1 means "very similar",
      - 0 means "unrelated",
      - -1 means "opposite".
    """
    # Make sure both vectors are NumPy arrays with decimal numbers.
    vec1 = np.asarray(vec1, dtype=np.float32)
    vec2 = np.asarray(vec2, dtype=np.float32)

    # Calculate the length (magnitude) of each vector.
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)

    # If either vector has length 0, we cannot compute a meaningful similarity.
    if norm1 == 0.0 or norm2 == 0.0:
        return 0.0

    # Compute the cosine similarity using the dot product formula.
    return float(np.dot(vec1, vec2) / (norm1 * norm2))


async def embed_texts(texts: List[str]) -> np.ndarray:
    """
    This function takes a list of text sentences and converts each one
    into a numeric vector using the SentenceTransformer model.
    These vectors let us mathematically compare meanings of sentences.
    """
    # If there are no texts, return an empty matrix with the correct number of columns.
    if not texts:
        dim = MODEL.get_sentence_embedding_dimension()
        return np.zeros((0, dim), dtype=np.float32)

    # Get the current event loop that runs asynchronous tasks.
    loop = asyncio.get_event_loop()

    # Run the embedding work in a background thread so it doesnâ€™t block other tasks.
    # MODEL.encode() converts each sentence into a numeric vector.
    return await loop.run_in_executor(
        EXECUTOR,
        lambda: MODEL.encode(texts, convert_to_numpy=True)
    )


# -------------------------
# Main evaluation function
# -------------------------

async def evaluate_llm_vs_user_semantic_only(
    llm_output: str,
    user_input: Dict[str, Any]
) -> Dict[str, Any]:
    """
    This function compares criteria generated by the LLM (AI model)
    with criteria written by a human, using only semantic (meaning-based) similarity.

    It returns a dictionary with:
      - overall_metrics: one summary per control (average similarity, hallucinations, etc.)
      - per_criterion_results: one entry per individual criterion pair.
    """

    # This is what we return if something goes wrong (e.g., an exception happens).
    default_result = {"overall_metrics": {}, "per_criterion_results": []}

    try:
        # -------------------------
        # Parse LLM output
        # -------------------------

        # Remove extra spaces or line breaks from the start and end of the LLM output.
        stripped = llm_output.strip()

        # If the output starts with ``` it means the model wrapped JSON in a code block.
        if stripped.startswith("```"):
            # Find the first line break (end of the ``` or ```json line).
            first_newline = stripped.find("\n")
            # If we find it, remove everything up to and including that line.
            if first_newline != -1:
                stripped = stripped[first_newline + 1:]
            # If the text now ends with ``` remove those last three characters.
            if stripped.endswith("```"):
                stripped = stripped[:-3]

        # Convert the cleaned JSON string to a Python object (list of dicts).
        llm_json = json.loads(stripped)

        # Get the list of human-written criteria from user_input.
        expected_content = user_input.get("compare_content", {}).get("expected_content", [])

        # If either the LLM output or the human content is empty, log a warning.
        if not llm_json or not expected_content:
            logger.warning("LLM output or expected content is empty")

        # Extract the text of each criterion from the LLM output.
        llm_texts = [item.get("criteria", "") for item in llm_json]

        # Extract the text of each human criterion from expected_content.
        user_texts = [item.get("criteria", "") for item in expected_content]

        # -------------------------
        # Generate embeddings
        # -------------------------

        # At the same time, convert LLM and human criteria into numeric vectors.
        # This runs in parallel using asyncio.gather.
        llm_embeds, user_embeds = await asyncio.gather(
            embed_texts(llm_texts),
            embed_texts(user_texts)
        )

        # Determine how many pairs we can compare.
        # We only compare up to the smallest count between LLM and human criteria.
        n_pairs = min(llm_embeds.shape[0], user_embeds.shape[0])

        # -------------------------
        # Compute similarities once
        # -------------------------

        # This list will store the similarity score for each (LLM, human) pair.
        pair_sims: List[float] = []

        # For each pair at the same index (1 with 1, 2 with 2, 3 with 3, etc.),
        # compute the semantic similarity and save it.
        for i in range(n_pairs):
            pair_sims.append(cosine_similarity(llm_embeds[i], user_embeds[i]))

        # -------------------------
        # Hallucination detection using pair_sims
        # -------------------------

        # This list will hold the indices of LLM outputs that we consider "hallucinated".
        hallucinations: List[int] = []

        # If there are no human embeddings at all, we treat every LLM output as hallucinated.
        if user_embeds.shape[0] == 0:
            hallucinations = list(range(llm_embeds.shape[0]))
        else:
            # For each pair, if the similarity is below the threshold,
            # mark that index as hallucinated (too different from the expected meaning).
            for i, sim in enumerate(pair_sims):
                if sim < SIMILARITY_THRESHOLD:
                    hallucinations.append(i)

            # If the LLM produced more criteria than the human side,
            # mark all extra ones as hallucinations because there is no reference to compare.
            if llm_embeds.shape[0] > n_pairs:
                hallucinations.extend(range(n_pairs, llm_embeds.shape[0]))

        # Count how many hallucinated criteria we found.
        hallucination_count = len(hallucinations)

        # Calculate what fraction of all LLM criteria are hallucinated.
        hallucination_rate = (
            hallucination_count / llm_embeds.shape[0]
            if llm_embeds is not None and llm_embeds.shape[0] > 0
            else 0.0
        )

        # -------------------------
        # Overall average similarity (per control/test_case)
        # -------------------------

        # Compute the average similarity across all compared pairs.
        # This gives us one summary score for the control.
        average_semantic_similarity = float(np.mean(pair_sims)) if pair_sims else 0.0

        # -------------------------
        # Per-criterion evaluation
        # -------------------------

        # This list will hold detailed info for each individual criterion pair.
        per_criterion_results = []

        # For each pair, build a result record that contains:
        # - the human criterion text
        # - the LLM criterion text
        # - their similarity score
        # - whether it's hallucinated or not
        for i in range(n_pairs):
            sim = pair_sims[i]
            per_criterion_results.append({
                "criteria_id": llm_json[i].get("id"),
                "human_criteria": user_texts[i],
                "llm_criteria": llm_texts[i],
                "framework": user_input.get("framework", ""),
                "control_id": user_input.get("control_id", ""),
                "control": user_input.get("control", ""),
                "subcontrol": user_input.get("subcontrol", ""),
                # The key number: how similar this pair of sentences is, on average.
                "semantic_similarity": round(sim, 4),
                # Whether we think this is too different from the human text.
                "is_hallucinated": i in hallucinations,
                # The threshold we used to decide hallucination.
                "semantic_similarity_threshold": SIMILARITY_THRESHOLD
            })

        # Now handle any extra LLM criteria that had no corresponding human criterion.
        # We mark them as hallucinated with 0 similarity.
        for i in range(n_pairs, llm_embeds.shape[0]):
            per_criterion_results.append({
                "criteria_id": llm_json[i].get("id"),
                "human_criteria": "",
                "llm_criteria": llm_texts[i],
                "framework": user_input.get("framework", ""),
                "control_id": user_input.get("control_id", ""),
                "control": user_input.get("control", ""),
                "subcontrol": user_input.get("subcontrol", ""),
                "semantic_similarity": 0.0,
                "is_hallucinated": True,
                "semantic_similarity_threshold": SIMILARITY_THRESHOLD
            })

        # -------------------------
        # Overall metrics (per test_case / control_id)
        # -------------------------

        # Build a summary of the results for this control/subcontrol/framework.
        overall_metrics = {
            # Identify which control this summary belongs to.
            "control_id": user_input.get("control_id", ""),
            "control": user_input.get("control", ""),
            "subcontrol": user_input.get("subcontrol", ""),
            "framework": user_input.get("framework", ""),

            # The main metric: average similarity across all criteria pairs.
            "average_semantic_similarity": round(average_semantic_similarity, 4),

            # How many criteria seem "hallucinated".
            "hallucination_count": hallucination_count,

            # The percentage of hallucinated criteria.
            "hallucination_rate": round(hallucination_rate, 4),

            # The list of indices that were flagged as hallucinated.
            "hallucinations": hallucinations
        }

        # Return both the overall summary and the detailed per-criterion results.
        return {
            "overall_metrics": overall_metrics,
            "per_criterion_results": per_criterion_results
        }

    except Exception:
        # If anything goes wrong in the try-block above, log the error with traceback.
        logger.exception("Evaluation failed")
        # And return the default empty result structure.
        return default_result


# -------------------------
# Demo main
# -------------------------

async def main_demo():
    """
    This is a small example to show how the evaluation works.
    It builds a fake user_input (with human criteria) and a fake llm_output,
    then runs the evaluation and prints the result.
    """

    # Example of input that describes one control, with 3 human-written criteria.
    user_input = {
        "control_id": "M1.1.1",
        "control": "Understanding The Entity and its Context",
        "subcontrol": "The entity shall determine interested parties that are relevant to its information security.",
        "framework": "UAE Information Assurance Regulation",
        "compare_content": {
            "expected_content": [
                {"id": 1, "criteria": "Identify stakeholders"},
                {"id": 2, "criteria": "Conduct stakeholder analysis"},
                {"id": 3, "criteria": "Establish communication channels"}
            ]
        }
    }

    # Example of LLM-generated criteria for the same control.
    # In real use, this would come from your LLM API call.
    llm_output_json = json.dumps([
        {"id": 1, "criteria": "Identify all relevant stakeholders affecting information security."},
        {"id": 2, "criteria": "Perform stakeholder analysis on a regular basis."},
        {"id": 3, "criteria": "Define and maintain communication channels with stakeholders."}
    ])

    # Call the evaluation function and wait for the result.
    result = await evaluate_llm_vs_user_semantic_only(llm_output_json, user_input)

    # Print the result in a nicely formatted JSON string.
    print(json.dumps(result, indent=2))


# This block runs only if this file is executed directly (not imported).
if __name__ == "__main__":
    # Run the demo example using Python's asyncio event loop.
    asyncio.run(main_demo())
