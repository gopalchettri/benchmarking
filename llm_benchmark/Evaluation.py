"""
Production-Grade Criteria Evaluator
Optimized for performance, reusability, and maintainability
"""

import asyncio
import json
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from difflib import SequenceMatcher
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class EvaluationConfig:
    """Configuration for evaluation thresholds and weights"""
    semantic_weight: float = 0.30
    exact_match_weight: float = 0.15
    completeness_weight: float = 0.30
    precision_weight: float = 0.15
    keyword_weight: float = 0.10
    semantic_threshold: float = 0.75
    exact_match_threshold: float = 0.90
    embedding_model: str = "all-MiniLM-L6-v2"
    batch_size: int = 32


@dataclass
class EvaluationResult:
    """Structured evaluation result"""
    control_id: str
    model: str
    framework: str
    semantic_similarity: float
    exact_match_score: float
    completeness_recall: float
    precision: float
    keyword_coverage: float
    f1_score: float
    composite_score: float
    expected_count: int
    generated_count: int
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return asdict(self)


class CriteriaEvaluator:
    """
    Production-grade evaluator for LLM-generated criteria
    
    Features:
    - Async execution with concurrent metric calculation
    - Batched embedding generation for performance
    - Configurable weights and thresholds
    - Type-safe with dataclasses
    - Reusable across different use cases
    - Comprehensive error handling
    """
    
    def __init__(self, config: Optional[EvaluationConfig] = None):
        """
        Initialize evaluator with optional custom configuration
        
        Args:
            config: EvaluationConfig object with custom settings
        """
        self.config = config or EvaluationConfig()
        self._encoder: Optional[SentenceTransformer] = None
        self._embedding_cache: Dict[str, np.ndarray] = {}
        logger.info(f"Initialized evaluator with model: {self.config.embedding_model}")
    
    @property
    def encoder(self) -> SentenceTransformer:
        """Lazy load sentence transformer model"""
        if self._encoder is None:
            logger.info("Loading sentence transformer model...")
            self._encoder = SentenceTransformer(self.config.embedding_model)
            logger.info("Model loaded successfully")
        return self._encoder
    
    async def evaluate_criteria(
        self, 
        generated_criteria: List[str], 
        test_case: Dict[str, Any],
        model_name: str
    ) -> EvaluationResult:
        """
        Main evaluation method - calculates all metrics asynchronously
        
        Args:
            generated_criteria: List of criteria generated by LLM
            test_case: Test case dict with compare_criteria field
            model_name: Name of model being benchmarked
        
        Returns:
            EvaluationResult object with all scores
        """
        # Validate inputs
        if not isinstance(generated_criteria, list):
            raise ValueError("generated_criteria must be a list")
        if not isinstance(test_case, dict):
            raise ValueError("test_case must be a dictionary")
        
        compare_data = test_case.get("compare_criteria", {})
        expected_criteria = compare_data.get("expected_criteria", [])
        
        # Handle empty cases
        if not generated_criteria or not expected_criteria:
            return self._create_empty_result(test_case, model_name)
        
        # Pre-compute embeddings once for reuse
        gen_embeddings, exp_embeddings = await self._compute_embeddings_batch(
            generated_criteria, 
            expected_criteria
        )
        
        # Run all metrics concurrently
        scores = await asyncio.gather(
            self._calculate_semantic_similarity(gen_embeddings, exp_embeddings),
            self._calculate_exact_match(generated_criteria, expected_criteria),
            self._calculate_completeness(gen_embeddings, exp_embeddings, compare_data),
            self._calculate_precision(gen_embeddings, exp_embeddings),
            self._calculate_keyword_coverage(generated_criteria, expected_criteria),  # ✅ CHANGE 1: Pass expected_criteria instead of compare_data
            return_exceptions=True
        )
        
        # Handle any exceptions from concurrent execution
        for i, score in enumerate(scores):
            if isinstance(score, Exception):
                logger.error(f"Error calculating metric {i}: {score}")
                scores[i] = 0.0
        
        # Unpack results
        semantic_sim, exact_match, completeness, precision, keyword_cov = scores
        
        # Calculate derived metrics
        f1_score = self._calculate_f1(precision, completeness)
        composite_score = self._calculate_composite(
            semantic_sim, exact_match, completeness, precision, keyword_cov
        )
        
        return EvaluationResult(
            control_id=test_case.get("control_id", "unknown"),
            model=model_name,
            framework=test_case.get("framework", ""),
            semantic_similarity=round(semantic_sim, 4),
            exact_match_score=round(exact_match, 4),
            completeness_recall=round(completeness, 4),
            precision=round(precision, 4),
            keyword_coverage=round(keyword_cov, 4),
            f1_score=round(f1_score, 4),
            composite_score=round(composite_score, 4),
            expected_count=compare_data.get("expected_count", len(expected_criteria)),
            generated_count=len(generated_criteria)
        )
    
    async def _compute_embeddings_batch(
        self, 
        generated: List[str], 
        expected: List[str]
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Compute embeddings in batches for better performance
        Uses caching to avoid recomputing identical texts
        """
        # Check cache first
        gen_embeddings = self._get_cached_embeddings(generated)
        exp_embeddings = self._get_cached_embeddings(expected)
        
        if gen_embeddings is None:
            gen_embeddings = self.encoder.encode(
                generated, 
                batch_size=self.config.batch_size,
                convert_to_tensor=False,
                show_progress_bar=False
            )
        
        if exp_embeddings is None:
            exp_embeddings = self.encoder.encode(
                expected,
                batch_size=self.config.batch_size,
                convert_to_tensor=False,
                show_progress_bar=False
            )
        
        return gen_embeddings, exp_embeddings
    
    def _get_cached_embeddings(self, texts: List[str]) -> Optional[np.ndarray]:
        """Check if embeddings are cached"""
        cache_key = "||".join(sorted(texts))
        return self._embedding_cache.get(cache_key)
    
    async def _calculate_semantic_similarity(
        self, 
        gen_embeddings: np.ndarray, 
        exp_embeddings: np.ndarray
    ) -> float:
        """Calculate average cosine similarity using pre-computed embeddings"""
        # Calculate pairwise similarities
        similarity_matrix = cosine_similarity(gen_embeddings, exp_embeddings)
        
        # Best match strategy: for each expected, find best generated match
        best_matches = similarity_matrix.max(axis=0)
        return float(np.mean(best_matches))
    
    async def _calculate_exact_match(
        self, 
        generated: List[str], 
        expected: List[str]
    ) -> float:
        """Calculate percentage of exact or near-exact matches"""
        matches = 0
        threshold = self.config.exact_match_threshold
        
        for exp in expected:
            exp_lower = exp.lower()
            for gen in generated:
                # Use SequenceMatcher for fuzzy matching
                ratio = SequenceMatcher(None, exp_lower, gen.lower()).ratio()
                if ratio >= threshold:
                    matches += 1
                    break
        
        return matches / len(expected) if expected else 0.0
    
    async def _calculate_completeness(
        self, 
        gen_embeddings: np.ndarray,
        exp_embeddings: np.ndarray,
        compare_data: Dict[str, Any]
    ) -> float:
        """Calculate recall using pre-computed embeddings"""
        expected_count = compare_data.get("expected_count", len(exp_embeddings))
        if expected_count == 0:
            return 0.0
        
        # Calculate similarity matrix
        similarity_matrix = cosine_similarity(exp_embeddings, gen_embeddings)
        
        # Count matches above threshold
        matches = (similarity_matrix.max(axis=1) > self.config.semantic_threshold).sum()
        
        return float(matches / expected_count)
    
    async def _calculate_precision(
        self, 
        gen_embeddings: np.ndarray,
        exp_embeddings: np.ndarray
    ) -> float:
        """Calculate precision using pre-computed embeddings"""
        if len(gen_embeddings) == 0:
            return 0.0
        
        # Calculate similarity matrix
        similarity_matrix = cosine_similarity(gen_embeddings, exp_embeddings)
        
        # Count correct generated criteria
        correct = (similarity_matrix.max(axis=1) > self.config.semantic_threshold).sum()
        
        return float(correct / len(gen_embeddings))
    
    async def _calculate_keyword_coverage(
        self, 
        generated: List[str], 
        expected: List[str]  # ✅ CHANGE 2: Changed parameter from compare_data to expected_criteria
    ) -> float:
        """
        ✅ CHANGE 3: Completely rewritten keyword coverage logic
        Extract key terms from expected criteria and check coverage in generated criteria
        """
        # Extract meaningful keywords from expected criteria (nouns, verbs, important terms)
        import re
        
        # Define common stop words to exclude
        stop_words = {
            'a', 'an', 'the', 'is', 'are', 'be', 'been', 'being', 'have', 'has', 'had',
            'do', 'does', 'did', 'will', 'would', 'should', 'could', 'may', 'might',
            'must', 'can', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',
            'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from',
            'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',
            'then', 'once', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',
            'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too',
            'very', 's', 't', 'just', 'don', 'now', 'based', 'using'
        }
        
        # Extract keywords from expected criteria
        expected_keywords = set()
        for criterion in expected:
            # Tokenize and clean
            words = re.findall(r'\b[a-z]{3,}\b', criterion.lower())
            # Filter out stop words
            meaningful_words = [w for w in words if w not in stop_words]
            expected_keywords.update(meaningful_words)
        
        if not expected_keywords:
            return 1.0  # No keywords to check
        
        # Check coverage in generated criteria
        generated_text = " ".join(generated).lower()
        keyword_found = sum(1 for kw in expected_keywords if kw in generated_text)
        
        return keyword_found / len(expected_keywords)
    
    def _calculate_f1(self, precision: float, recall: float) -> float:
        """Calculate F1 score from precision and recall"""
        if precision + recall == 0:
            return 0.0
        return 2 * (precision * recall) / (precision + recall)
    
    def _calculate_composite(
        self, 
        semantic_sim: float, 
        exact_match: float,
        completeness: float, 
        precision: float, 
        keyword_cov: float
    ) -> float:
        """Calculate weighted composite score"""
        return (
            self.config.semantic_weight * semantic_sim +
            self.config.exact_match_weight * exact_match +
            self.config.completeness_weight * completeness +
            self.config.precision_weight * precision +
            self.config.keyword_weight * keyword_cov
        )
    
    def _create_empty_result(
        self, 
        test_case: Dict[str, Any], 
        model_name: str
    ) -> EvaluationResult:
        """Create zero-score result for empty inputs"""
        return EvaluationResult(
            control_id=test_case.get("control_id", "unknown"),
            model=model_name,
            framework=test_case.get("framework", ""),
            semantic_similarity=0.0,
            exact_match_score=0.0,
            completeness_recall=0.0,
            precision=0.0,
            keyword_coverage=0.0,
            f1_score=0.0,
            composite_score=0.0,
            expected_count=0,
            generated_count=0
        )
    
    def save_results(
        self, 
        result: EvaluationResult, 
        output_path: Path
    ) -> None:
        """
        Save evaluation result to JSONL file
        
        Args:
            result: EvaluationResult object
            output_path: Path to output file
        """
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'a') as f:
            json.dump(result.to_dict(), f)
            f.write('\n')
    
    def clear_cache(self) -> None:
        """Clear embedding cache to free memory"""
        self._embedding_cache.clear()
        logger.info("Embedding cache cleared")


# ============================================================================
# REUSABLE FACTORY FUNCTION FOR DIFFERENT USE CASES
# ============================================================================

def create_evaluator_for_use_case(use_case: str) -> CriteriaEvaluator:
    """
    Factory function to create evaluator with use-case-specific config
    
    Args:
        use_case: One of ['grc', 'qa', 'summarization', 'translation', 'general']
    
    Returns:
        Configured CriteriaEvaluator instance
    """
    configs = {
        'grc': EvaluationConfig(
            # GRC: Prioritize completeness (no missing controls)
            semantic_weight=0.30,
            completeness_weight=0.35,
            precision_weight=0.20,
            exact_match_weight=0.10,
            keyword_weight=0.05
        ),
        'qa': EvaluationConfig(
            # QA: Prioritize precision (no hallucinations)
            semantic_weight=0.35,
            precision_weight=0.35,
            completeness_weight=0.20,
            exact_match_weight=0.05,
            keyword_weight=0.05
        ),
        'summarization': EvaluationConfig(
            # Summarization: Balance precision and recall
            semantic_weight=0.40,
            precision_weight=0.25,
            completeness_weight=0.25,
            exact_match_weight=0.05,
            keyword_weight=0.05
        ),
        'translation': EvaluationConfig(
            # Translation: Prioritize semantic similarity
            semantic_weight=0.50,
            exact_match_weight=0.25,
            precision_weight=0.15,
            completeness_weight=0.10,
            keyword_weight=0.00
        ),
        'general': EvaluationConfig()  # Default balanced config
    }
    
    config = configs.get(use_case.lower(), EvaluationConfig())
    logger.info(f"Created evaluator for use case: {use_case}")
    return CriteriaEvaluator(config)


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

async def example_usage():
    """Demonstrate how to use the evaluator"""
    
    # Create evaluator for GRC use case
    evaluator = create_evaluator_for_use_case('grc')
    
    # ✅ CHANGE 4: Updated example test case to match new structure (removed mandatory_keywords)
    test_case = {
        "control_id": "IA-1.1.1",
        "control": "Access Control",
        "framework": "UAE IAR",
        "compare_criteria": {
            "expected_criteria": [
                "All users must be uniquely identified",
                "Access rights are granted based on least privilege",
                "Multi-factor authentication is enforced for privileged accounts"
            ],
            "expected_count": 3
        }
    }
    
    # Example generated criteria
    generated_criteria = [
        "Every user shall have a unique identifier",
        "Access permissions follow least privilege principle",
        "Privileged users must use two-factor authentication"
    ]
    
    # Evaluate
    result = await evaluator.evaluate_criteria(
        generated_criteria=generated_criteria,
        test_case=test_case,
        model_name="Phi-4-14B"
    )
    
    # Print results
    print(f"\nEvaluation Results for {result.control_id}")
    print(f"Model: {result.model}")
    print(f"Semantic Similarity: {result.semantic_similarity}")
    print(f"Exact Match Score: {result.exact_match_score}")
    print(f"Completeness (Recall): {result.completeness_recall}")
    print(f"Precision: {result.precision}")
    print(f"Keyword Coverage: {result.keyword_coverage}")
    print(f"F1 Score: {result.f1_score}")
    print(f"Composite Score: {result.composite_score}")
    print(f"Expected Count: {result.expected_count}")
    print(f"Generated Count: {result.generated_count}")
    
    # Save to file
    output_path = Path("results/phi4_scores.jsonl")
    evaluator.save_results(result, output_path)
    print(f"\nResults saved to {output_path}")


if __name__ == "__main__":
    asyncio.run(example_usage())
