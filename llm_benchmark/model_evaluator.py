"""
Evaluator for LLM-generated content
Optimized for performance, reusability, and maintainability
"""
import ssl
import certifi

from src.core.utils import load_sentence_transformer

ssl._create_default_https_context = ssl._create_unverified_context


import asyncio
import json
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from difflib import SequenceMatcher
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

model_dir="src/config/embedding_model"
model_name="all-MiniLM-L6-V2"


@dataclass
class EvaluationConfig:
    """Configuration for evaluation thresholds and weights"""
    semantic_weight: float = 0.30
    exact_match_weight: float = 0.15
    completeness_weight: float = 0.30
    precision_weight: float = 0.15
    keyword_weight: float = 0.10
    semantic_threshold: float = 0.75
    exact_match_threshold: float = 0.90
    embedding_model: str = "all-MiniLM-L6-v2"
    batch_size: int = 32


@dataclass
class EvaluationResult:
    """Structured evaluation result"""
    control_id: str
    model: str
    framework: str
    semantic_similarity: float
    exact_match_score: float
    completeness_recall: float
    precision: float
    keyword_coverage: float
    f1_score: float
    composite_score: float
    expected_count: int
    generated_count: int
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return asdict(self)

class ModelEvaluator:
    """
    Evaluator for LLM-generated content
    
    Features:
    - Async execution with concurrent metric calculation
    - Batched embedding generation for performance
    - Configurable weights and thresholds
    - Type-safe with dataclasses
    - Reusable across different use cases
    - Comprehensive error handling
    """
    
    def __init__(self, config: Optional[EvaluationConfig] = None, model_directory:str=model_dir):
        """
        Initialize evaluator with optional custom configuration
        
        Args:
            config: EvaluationConfig object with custom settings
        """
        self.config = config or EvaluationConfig()
        self.model_dir = model_directory
        #self._encoder: Optional[SentenceTransformer] = None
        self._encoder = None
        self._embedding_cache: Dict[str, np.ndarray] = {}
        logger.info(f"Initialized evaluator with model in {model_dir}")
    
    @property
    async def embedding_model(self) -> SentenceTransformer:
        """Lazy load sentence transformer model"""
        if self._encoder is None:
            logger.info("Loading sentence transformer model...")
            #self._encoder = await load_sentence_transformer(self.model_dir, self.embedding_model)
            self._encoder = await load_sentence_transformer(model_dir, model_name)
            #local_modal_path = await load_sentence_transformer( self.model_dir, self.config.embedding_model,)
            #local_modal_path = await load_sentence_transformer( model_dir, model_name)
            
            #self._encoder = SentenceTransformer(self.config.embedding_model)
            #self._encoder = SentenceTransformer(local_modal_path)
            logger.info("Model loaded successfully from local directory.")
        return self._encoder
    
    async def evaluate_criteria(
        self, 
        llm_content: List[str], 
        test_case: Dict[str, Any],
        model_name: str
    ) -> EvaluationResult:
        """
        Main evaluation method - calculates all metrics asynchronously
        
        Args:
            generated_criteria: List of criteria/question/content generated by LLM
            test_case: Test case dict with compare_criteria field
            model_name: Name of model being benchmarked
        
        Returns:
            EvaluationResult object with all scores
        """
        # Validate inputs
        if not isinstance(llm_content, list):
            raise ValueError("llm_content must be a list")
        if not isinstance(test_case, dict):
            raise ValueError("test_case must be a dictionary")
        
        # Defined in the testcase
        compare_content = test_case.get("compare_content", {})         
        expected_content= compare_content.get("expected_content", [])
        
        # Handle empty cases
        if not llm_content or not compare_content:
            return self._create_empty_result(test_case, model_name)
        
        # Pre-compute embeddings once for reuse
        llm_generated_content_embeddings, expected_content_embeddings = await self._compute_embeddings_batch(
            llm_content, 
            expected_content
        )
        
        # Run all metrics concurrently
        scores = await asyncio.gather(
            self._calculate_semantic_similarity(llm_generated_content_embeddings, expected_content_embeddings),
            self._calculate_exact_match(llm_content, expected_content),
            self._calculate_completeness(llm_generated_content_embeddings, expected_content_embeddings, compare_content),
            self._calculate_precision(llm_generated_content_embeddings, expected_content_embeddings),
            self._calculate_keyword_coverage(llm_content, expected_content),  
            return_exceptions=True
        )
        
        # Handle any exceptions from concurrent execution
        for i, score in enumerate(scores):
            if isinstance(score, Exception):
                logger.error(f"Error calculating metric {i}: {score}")
                scores[i] = 0.0
        
        # Unpack results
        semantic_sim, exact_match, completeness, precision, keyword_cov = scores
        
        # Calculate derived metrics
        f1_score = self._calculate_f1(precision, completeness)
        composite_score = self._calculate_composite(
            semantic_sim, exact_match, completeness, precision, keyword_cov
        )
        
        return EvaluationResult(
            control_id=test_case.get("control_id", "unknown"),
            model=model_name,
            framework=test_case.get("framework", ""),
            semantic_similarity=round(semantic_sim, 4),
            exact_match_score=round(exact_match, 4),
            completeness_recall=round(completeness, 4),
            precision=round(precision, 4),
            keyword_coverage=round(keyword_cov, 4),
            f1_score=round(f1_score, 4),
            composite_score=round(composite_score, 4),
            expected_count=compare_content.get("expected_count", len(expected_content)),
            generated_count=len(llm_content)
        )
    
    async def _compute_embeddings_batch(
        self, 
        llm_generated: List[str], 
        expected: List[str]
    ) :
        """
        Compute embeddings in batches for better performance
        Uses caching to avoid recomputing identical texts
        """

        try:

            # Initialize variables to prevent UnboundeLocalError
            llm_gen_embeddings =  None
            exp_embeddings= None

            # Get the actual encoder object by awaiting the property
            embedding_model = await self.embedding_model 
            #self.encoder

            # Check cache first
            llm_gen_embeddings_cached = self._get_cached_embeddings(llm_generated)
            exp_embeddings_cached = self._get_cached_embeddings(expected)
            
            # Generate embeddings if not in cache
            if llm_gen_embeddings_cached is None:
                # Get the actual encoder object (await the property)
                llm_gen_embeddings = embedding_model.encode(
                    llm_generated, 
                    batch_size=self.config.batch_size,
                    convert_to_tensor=False,
                    show_progress_bar=False
                )
            else:
                llm_gen_embeddings= llm_gen_embeddings_cached
            
            # Generate exp embeddings if not cached.
            if exp_embeddings_cached is None:
                # Get the actual encoder object (await the property)
                exp_embeddings = embedding_model.encode(
                    expected,
                    batch_size=self.config.batch_size,
                    convert_to_tensor=False,
                    show_progress_bar=False
                )
            else:
                exp_embeddings = exp_embeddings_cached

            # Final Validation
            if llm_gen_embeddings is None:
                raise ValueError("Failed to generate embeddings for LLM generated content in _compute_embeddings_batch function ")
            
            if exp_embeddings is None:
                raise ValueError("Falied to generate embeddings for expected in _compute_embeddings_batch.")
            return llm_gen_embeddings, exp_embeddings
        except Exception as e:
            logger.error(f"_compute_embeddings_batch: {e}", exc_info=True)
            raise 

    
    def _get_cached_embeddings(self, texts: List[str]) -> Optional[np.ndarray]:
        """Check if embeddings are cached"""
        cache_key = "||".join(sorted(texts))
        return self._embedding_cache.get(cache_key)
    
    async def _calculate_semantic_similarity(
        self, 
        llm_gen_embeddings: np.ndarray, 
        exp_embeddings: np.ndarray
    ) -> float:
        """Calculate average cosine similarity using pre-computed embeddings"""
        # Calculate pairwise similarities
        similarity_matrix = cosine_similarity(llm_gen_embeddings, exp_embeddings)
        
        # Best match strategy: for each expected, find best generated match
        best_matches = similarity_matrix.max(axis=0)
        return float(np.mean(best_matches))
    
    async def _calculate_exact_match(
        self, 
        llm_generated: List[str], 
        expected: List[str]
    ) -> float:
        """Calculate percentage of exact or near-exact matches"""
        matches = 0
        threshold = self.config.exact_match_threshold
        
        for exp in expected:
            exp_lower = exp.lower()
            for gen in llm_generated:
                # Use SequenceMatcher for fuzzy matching
                ratio = SequenceMatcher(None, exp_lower, gen.lower()).ratio()
                if ratio >= threshold:
                    matches += 1
                    break
        
        return matches / len(expected) if expected else 0.0
    
    async def _calculate_completeness(
        self, 
        llm_gen_embeddings: np.ndarray,
        exp_embeddings: np.ndarray,
        compare_content: Dict[str, Any]
    ) -> float:
        """Calculate recall using pre-computed embeddings"""
        expected_count = compare_content.get("expected_count", len(exp_embeddings))
        if expected_count == 0:
            return 0.0
        
        # Calculate similarity matrix
        similarity_matrix = cosine_similarity(exp_embeddings, llm_gen_embeddings)
        
        # Count matches above threshold
        matches = (similarity_matrix.max(axis=1) > self.config.semantic_threshold).sum()
        
        return float(matches / expected_count)
    
    async def _calculate_precision(
        self, 
        llm_gen_embeddings: np.ndarray,
        exp_embeddings: np.ndarray
    ) -> float:
        """Calculate precision using pre-computed embeddings"""
        if len(llm_gen_embeddings) == 0:
            return 0.0
        
        # Calculate similarity matrix
        similarity_matrix = cosine_similarity(llm_gen_embeddings, exp_embeddings)
        
        # Count correct generated criteria
        correct = (similarity_matrix.max(axis=1) > self.config.semantic_threshold).sum()
        
        return float(correct / len(llm_gen_embeddings))
    
    async def _calculate_keyword_coverage(
        self, 
        llm_generated: List[str], 
        expected: List[str]  
    ) -> float:
        """        
        Extract key terms from expected criteria and check coverage in llm generated content
        """
        # Extract meaningful keywords from expected criteria (nouns, verbs, important terms)
        import re
        
        # Define common stop words to exclude  # replace with spacy or other library
        stop_words = {
            'a', 'an', 'the', 'is', 'are', 'be', 'been', 'being', 'have', 'has', 'had',
            'do', 'does', 'did', 'will', 'would', 'should', 'could', 'may', 'might',
            'must', 'can', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',
            'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from',
            'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',
            'then', 'once', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',
            'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too',
            'very', 's', 't', 'just', 'don', 'now', 'based', 'using'
        }
        
        # Extract keywords from expected criteria
        expected_keywords = set()
        for criterion in expected:
            # Tokenize and clean
            words = re.findall(r'\b[a-z]{3,}\b', criterion.lower())
            # Filter out stop words
            meaningful_words = [w for w in words if w not in stop_words]
            expected_keywords.update(meaningful_words)
        
        if not expected_keywords:
            return 1.0  # No keywords to check
        
        # Check coverage in generated criteria
        llm_generated_text = " ".join(llm_generated).lower()
        keyword_found = sum(1 for kw in expected_keywords if kw in llm_generated_text)
        
        return keyword_found / len(expected_keywords)
    
    def _calculate_f1(self, precision: float, recall: float) -> float:
        """Calculate F1 score from precision and recall"""
        if precision + recall == 0:
            return 0.0
        return 2 * (precision * recall) / (precision + recall)
    
    def _calculate_composite(
        self, 
        semantic_sim: float, 
        exact_match: float,
        completeness: float, 
        precision: float, 
        keyword_cov: float
    ) -> float:
        """Calculate weighted composite score"""
        return (
            self.config.semantic_weight * semantic_sim +
            self.config.exact_match_weight * exact_match +
            self.config.completeness_weight * completeness +
            self.config.precision_weight * precision +
            self.config.keyword_weight * keyword_cov
        )
    
    def _create_empty_result(
        self, 
        test_case: Dict[str, Any], 
        model_name: str
    ) -> EvaluationResult:
        """Create zero-score result for empty inputs"""
        return EvaluationResult(
            control_id=test_case.get("control_id", "unknown"),
            model=model_name,
            framework=test_case.get("framework", ""),
            semantic_similarity=0.0,
            exact_match_score=0.0,
            completeness_recall=0.0,
            precision=0.0,
            keyword_coverage=0.0,
            f1_score=0.0,
            composite_score=0.0,
            expected_count=0,
            generated_count=0
        )
    
    def save_results(
        self, 
        result: EvaluationResult, 
        output_path: Path
    ) -> None:
        """
        Save evaluation result to JSONL file
        
        Args:
            result: EvaluationResult object
            output_path: Path to output file
        """
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'a') as f:
            json.dump(result.to_dict(), f)
            f.write('\n')
    
    def clear_cache(self) -> None:
        """Clear embedding cache to free memory"""
        self._embedding_cache.clear()
        logger.info("Embedding cache cleared")


# ============================================================================
# REUSABLE FACTORY FUNCTION FOR DIFFERENT USE CASES
# ============================================================================

async def create_evaluator_for_use_case(use_case: str) -> ModelEvaluator:
    """
    Factory function to create evaluator with use-case-specific config
    
    Args:
        use_case: One of ['criteria', 'question', 'evidence', 'policy', 'general']
    
    Returns:
        Configured CriteriaEvaluator instance
    """
    configs = {
        'criteria': EvaluationConfig(
            # criteria: Prioritize completeness (no missing controls)
            semantic_weight=0.35,
            completeness_weight=0.35,
            precision_weight=0.20,
            exact_match_weight=0.05,
            keyword_weight=0.05
        ),
        'question': EvaluationConfig(
            # Q: Prioritize precision (no hallucinations)
            semantic_weight=0.35,
            precision_weight=0.35,
            completeness_weight=0.20,
            exact_match_weight=0.05,
            keyword_weight=0.05
        ),
        'evidence': EvaluationConfig(
            # Evidence Analyzer: Balance precision and recall
            semantic_weight=0.55,
            precision_weight=0.25,
            completeness_weight=0.10,
            exact_match_weight=0.05,
            keyword_weight=0.05
        ),
        'policy': EvaluationConfig(
            # Policy ChatBot: Prioritize semantic similarity
            semantic_weight=0.50,
            exact_match_weight=0.25,
            precision_weight=0.15,
            completeness_weight=0.05,
            keyword_weight=0.05
        ),
        'general': EvaluationConfig()  # Default balanced config
    }
    
    config = configs.get(use_case.lower(), EvaluationConfig())
    logger.info(f"Created evaluator for use case: {use_case}")
    return ModelEvaluator(config)


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

async def example_usage():
    """Demonstrate how to use the evaluator"""
    
    # Create evaluator for criteria use case
    evaluator = create_evaluator_for_use_case('criteria')
    
    # Sample test_case
    test_case = {
        "control_id": "IA-1.1.1",
        "control": "Access Control",
        "framework": "UAE IAR",
        "compare_content": {
            "expected_content": [
                "All users must be uniquely identified",
                "Access rights are granted based on least privilege",
                "Multi-factor authentication is enforced for privileged accounts"
            ],
            "expected_count": 3
        }
    }
    
    # Example llm generated criteria
    llm_generated_criteria = [
        "Every user shall have a unique identifier",
        "Access permissions follow least privilege principle",
        "Privileged users must use two-factor authentication"
    ]
    
    # Evaluate
    result = await evaluator.evaluate_criteria(
        llm_generated_content=llm_generated_criteria,
        test_case=test_case,
        model_name="Phi-4-14B"
    )
    
    # Print results
    print(f"\nEvaluation Results for {result.control_id}")
    print(f"Model: {result.model}")
    print(f"Semantic Similarity: {result.semantic_similarity}")
    print(f"Exact Match Score: {result.exact_match_score}")
    print(f"Completeness (Recall): {result.completeness_recall}")
    print(f"Precision: {result.precision}")
    print(f"Keyword Coverage: {result.keyword_coverage}")
    print(f"F1 Score: {result.f1_score}")
    print(f"Composite Score: {result.composite_score}")
    print(f"Expected Count: {result.expected_count}")
    print(f"Generated Count: {result.generated_count}")
    
    # Save to file
    output_path = Path("results/phi4_scores.jsonl")
    evaluator.save_results(result, output_path)
    print(f"\nResults saved to {output_path}")


# if __name__ == "__main__":
#     asyncio.run(example_usage())
